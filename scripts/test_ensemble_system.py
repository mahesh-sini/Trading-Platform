#!/usr/bin/env python3
"""
Test Ensemble ML System
Quick test to validate the ensemble system with sample data
"""

import asyncio
import logging
import sys
import os
from datetime import datetime, timedelta, date

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.services.bhav_copy_fetcher import bhav_fetcher
from backend.services.ensemble_ml_system import ensemble_ml_system, ModelType

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnsembleSystemTester:
    """Test the ensemble ML system functionality"""
    
    def __init__(self):
        self.test_symbols = [
            ("RELIANCE", "NSE"),
            ("TCS", "NSE"),
            ("HDFCBANK", "NSE"),
            ("INFY", "NSE"),
            ("ICICIBANK", "NSE")
        ]
    
    async def test_data_download(self):
        """Test bhav copy data download"""
        logger.info("ğŸ§ª Testing Bhav Copy Data Download")
        logger.info("-" * 50)
        
        try:
            # Test download for last 5 trading days
            end_date = date.today() - timedelta(days=1)
            start_date = end_date - timedelta(days=7)  # Get a week to ensure 5 trading days
            
            logger.info(f"ğŸ“… Testing download for: {start_date} to {end_date}")
            
            # Download test data
            stats = await bhav_fetcher.download_historical_bhav_data(start_date, end_date)
            
            logger.info("ğŸ“Š Download Test Results:")
            logger.info(f"  ğŸ“… Days processed: {stats['total_days']}")
            logger.info(f"  âœ… Successful days: {stats['successful_days']}")
            logger.info(f"  ğŸ“ˆ Total records: {stats['total_records']}")
            logger.info(f"  ğŸ›ï¸ NSE records: {stats['nse_records']}")
            logger.info(f"  ğŸ›ï¸ BSE records: {stats['bse_records']}")
            
            if stats['errors']:
                logger.warning(f"  âš ï¸ Errors: {len(stats['errors'])}")
                for error in stats['errors'][:3]:  # Show first 3 errors
                    logger.warning(f"    â€¢ {error}")
            
            # Validate minimum data
            if stats['total_records'] > 1000:
                logger.info("âœ… Data download test: PASSED")
                return True
            else:
                logger.error("âŒ Data download test: FAILED - Insufficient data")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Data download test failed: {e}")
            return False
    
    async def test_ensemble_training(self):
        """Test ensemble model training"""
        logger.info("\nğŸ§ª Testing Ensemble Model Training")
        logger.info("-" * 50)
        
        try:
            successful_trainings = 0
            
            for symbol, exchange in self.test_symbols:
                logger.info(f"ğŸ¤– Testing training for {symbol} ({exchange})")
                
                try:
                    # Test training for this symbol
                    result = await ensemble_ml_system.train_ensemble_for_symbol(symbol, exchange)
                    
                    if "error" not in result:
                        successful_models = result.get('successful_models', 0)
                        total_models = result.get('total_models', 4)
                        
                        logger.info(f"  âœ… {symbol}: {successful_models}/{total_models} models trained")
                        
                        if successful_models >= 2:  # At least 2 models should work
                            successful_trainings += 1
                    else:
                        logger.error(f"  âŒ {symbol}: {result['error']}")
                
                except Exception as e:
                    logger.error(f"  âŒ {symbol}: Training failed - {e}")
            
            success_rate = (successful_trainings / len(self.test_symbols)) * 100
            
            logger.info(f"\nğŸ“Š Training Test Results:")
            logger.info(f"  âœ… Successful symbols: {successful_trainings}/{len(self.test_symbols)}")
            logger.info(f"  ğŸ“ˆ Success rate: {success_rate:.1f}%")
            
            if success_rate >= 60:  # 60% success rate acceptable for test
                logger.info("âœ… Ensemble training test: PASSED")
                return True
            else:
                logger.error("âŒ Ensemble training test: FAILED")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Ensemble training test failed: {e}")
            return False
    
    async def test_predictions(self):
        """Test ensemble predictions"""
        logger.info("\nğŸ§ª Testing Ensemble Predictions")
        logger.info("-" * 50)
        
        try:
            successful_predictions = 0
            
            for symbol, exchange in self.test_symbols:
                logger.info(f"ğŸ”® Testing prediction for {symbol} ({exchange})")
                
                try:
                    # Get ensemble prediction
                    prediction = await ensemble_ml_system.get_ensemble_prediction(symbol, exchange)
                    
                    if prediction:
                        logger.info(f"  âœ… {symbol}:")
                        logger.info(f"    Prediction: {prediction.final_prediction:.6f}")
                        logger.info(f"    Confidence: {prediction.confidence:.3f}")
                        logger.info(f"    Models used: {len(prediction.model_predictions)}")
                        logger.info(f"    Weights: {prediction.meta_learner_weights}")
                        
                        successful_predictions += 1
                    else:
                        logger.warning(f"  âš ï¸ {symbol}: No prediction available")
                
                except Exception as e:
                    logger.error(f"  âŒ {symbol}: Prediction failed - {e}")
            
            success_rate = (successful_predictions / len(self.test_symbols)) * 100
            
            logger.info(f"\nğŸ“Š Prediction Test Results:")
            logger.info(f"  âœ… Successful predictions: {successful_predictions}/{len(self.test_symbols)}")
            logger.info(f"  ğŸ“ˆ Success rate: {success_rate:.1f}%")
            
            if success_rate >= 40:  # 40% success rate acceptable for initial test
                logger.info("âœ… Prediction test: PASSED")
                return True
            else:
                logger.error("âŒ Prediction test: FAILED")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Prediction test failed: {e}")
            return False
    
    async def test_data_quality(self):
        """Test data quality and availability"""
        logger.info("\nğŸ§ª Testing Data Quality")
        logger.info("-" * 50)
        
        try:
            # Get data quality report
            quality_report = bhav_fetcher.get_data_quality_report()
            
            if quality_report:
                logger.info("ğŸ“Š Data Quality Report:")
                logger.info(f"  ğŸ“ˆ Total records: {quality_report.get('total_records', 0)}")
                logger.info(f"  ğŸ¢ Unique symbols: {quality_report.get('unique_symbols', 0)}")
                logger.info(f"  ğŸ“… Date range: {quality_report.get('date_range', 'Unknown')}")
                logger.info(f"  âœ… Symbols with sufficient data: {quality_report.get('symbols_with_sufficient_data', 0)}")
                
                # Check by exchange
                if quality_report.get('by_exchange'):
                    logger.info("  ğŸ“Š By Exchange:")
                    for exchange_data in quality_report['by_exchange']:
                        logger.info(f"    {exchange_data['exchange']}: {exchange_data['records']} records, {exchange_data['symbols']} symbols")
                
                # Validate quality metrics
                total_records = quality_report.get('total_records', 0)
                sufficient_symbols = quality_report.get('symbols_with_sufficient_data', 0)
                
                if total_records > 10000 and sufficient_symbols > 20:
                    logger.info("âœ… Data quality test: PASSED")
                    return True
                else:
                    logger.error("âŒ Data quality test: FAILED - Insufficient data quality")
                    return False
            else:
                logger.error("âŒ Data quality test: FAILED - No data available")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Data quality test failed: {e}")
            return False
    
    async def run_all_tests(self):
        """Run all ensemble system tests"""
        logger.info("ğŸš€ Starting Ensemble System Tests")
        logger.info("=" * 80)
        
        test_results = {}
        
        # Test 1: Data Download
        test_results['data_download'] = await self.test_data_download()
        
        # Test 2: Data Quality  
        test_results['data_quality'] = await self.test_data_quality()
        
        # Test 3: Ensemble Training
        test_results['ensemble_training'] = await self.test_ensemble_training()
        
        # Test 4: Predictions
        test_results['predictions'] = await self.test_predictions()
        
        # Summary
        self.print_test_summary(test_results)
        
        return test_results
    
    def print_test_summary(self, results: dict):
        """Print comprehensive test summary"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ ENSEMBLE SYSTEM TEST SUMMARY")
        logger.info("=" * 80)
        
        passed_tests = sum(1 for result in results.values() if result)
        total_tests = len(results)
        success_rate = (passed_tests / total_tests) * 100
        
        logger.info(f"ğŸ“Š Overall Results: {passed_tests}/{total_tests} tests passed ({success_rate:.1f}%)")
        logger.info("")
        
        # Individual test results
        for test_name, result in results.items():
            status = "âœ… PASSED" if result else "âŒ FAILED"
            test_display = test_name.replace('_', ' ').title()
            logger.info(f"  {test_display}: {status}")
        
        logger.info("")
        
        if success_rate >= 75:
            logger.info("ğŸ‰ SYSTEM STATUS: READY FOR PRODUCTION")
            logger.info("   All critical tests passed. System is production-ready.")
        elif success_rate >= 50:
            logger.info("âš ï¸ SYSTEM STATUS: NEEDS ATTENTION")
            logger.info("   Some tests failed. Review issues before production deployment.")
        else:
            logger.info("âŒ SYSTEM STATUS: NOT READY")
            logger.info("   Major issues detected. System needs fixes before use.")
        
        logger.info("")
        logger.info("ğŸ“‹ Next Steps:")
        if results.get('data_download', False):
            logger.info("  âœ… Data pipeline is working")
        else:
            logger.info("  âŒ Fix data download issues first")
        
        if results.get('ensemble_training', False):
            logger.info("  âœ… Model training is working")
        else:
            logger.info("  âŒ Check model training dependencies")
        
        if results.get('predictions', False):
            logger.info("  âœ… Prediction system is working")
        else:
            logger.info("  âŒ Debug prediction pipeline")
        
        logger.info("=" * 80)

async def main():
    """Main test function"""
    try:
        tester = EnsembleSystemTester()
        results = await tester.run_all_tests()
        
        # Return appropriate exit code
        passed_tests = sum(1 for result in results.values() if result)
        if passed_tests >= len(results) * 0.75:  # 75% pass rate
            return 0
        else:
            return 1
        
    except Exception as e:
        logger.error(f"âŒ Test suite failed: {e}")
        return 1

if __name__ == "__main__":
    exit(asyncio.run(main()))